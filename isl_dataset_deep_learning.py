# -*- coding: utf-8 -*-
"""ISL Dataset Deep Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U-4LSgqTZ8zL6T3Tykl0r8XnmljQ-L_-
"""

!wget -O dataset.zip "https://data.mendeley.com/public-files/datasets/kcmpdxky7p/files/c14c8953-be53-4c7a-85d5-5bcd7e79d86e/file_downloaded"

!unzip dataset.zip -d dataset_folder

!pip install tensorflow==2.12
!pip install tensorflow-gpu==2.12
!pip install opencv-python
!pip install mediapipe
!pip install scikit-learn
!pip install matplotlib

!pip install mediapipe

import cv2
import numpy as np
import os
from  matplotlib import pyplot as plt
import mediapipe as mp
import time

import cv2
import mediapipe as mp
from IPython.display import display, Javascript
from google.colab.output import eval_js

# Initialize mediapipe holistic model
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

# Function to start camera using JavaScript
def start_camera():
    js = Javascript("""
    async function startVideo() {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: true
      });
      const video = document.createElement('video');
      video.srcObject = stream;
      video.play();
      document.body.appendChild(video);
      return new Promise(resolve => {
        video.onloadedmetadata = () => {
          resolve(video);
        };
      });
    }
    startVideo();
    """)
    display(js)
start_camera()

from IPython.display import display, Javascript

def end_camera():
  """Stops the camera and removes the video element from the DOM."""
  js = Javascript("""
  const video = document.querySelector('video');
  if (video) {
    video.srcObject.getTracks().forEach(track => track.stop());
    video.remove();
  }
  """)
  display(js)

mp_holistic = mp.solutions.holistic # Holistic model
mp_drawing = mp.solutions.drawing_utils # Drawing utilities

cap = cv2.VideoCapture(0)
while cap.isOpened():
  ret, frame = cap.read()
  cv2.imshow('OpenCV Feed', frame)
  if cv2.waitKey(10) & 0xFF == ord('q'):
    break
cap.release()
cv2.destroyAllWindows()

def mediapipe_detection(image, model):
  image = cv2.cvColor(image, cv2.COLOUR_BGR2RGB)
  image.flags.writable = False
  results = model.process(image)
  image.flags.writeable = True
  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
  return image, results

def draw_landmarks(image, results):
    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections
    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections
    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections
    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections

def draw_styled_landmarks(image, results):
    # Draw face connections
    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS,
                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),
                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)
                             )
    # Draw pose connections
    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,
                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),
                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)
                             )
    # Draw left hand connections
    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),
                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)
                             )
    # Draw right hand connections
    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),
                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)
                             )

cap = cv2.VideoCapture(0)
# Initialize holistic model outside the loop
with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
  while cap.isOpened():
    print("Done")
    ret, frame = cap.read()
    # Pass 'holistic' as the model to mediapipe_detection
    image, results = mediapipe_detection(frame, holistic)

    # Process and use 'results' here, inside the loop
    draw_landmarks(frame, results)
    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    # For example:
    if results.left_hand_landmarks:
        print(len(results.left_hand_landmarks.landmark))
    else:
        print("No left hand detected")

    cv2.imshow('Open CV feed', frame)

    if cv2.waitKey(10) & 0xFF == ord('q'):
      break

cap.release()
cv2.destroyAllWindows()

# Import necessary libraries
import cv2
import mediapipe as mp
import numpy as np
from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

# Function to capture an image from the webcam
def capture_image():
    js = Javascript('''
    async function captureImage() {
        const video = document.createElement('video');
        const stream = await navigator.mediaDevices.getUserMedia({video: true});
        video.srcObject = stream;
        await video.play();

        // Resize the output to match the video stream's dimensions
        google.colab.output.setIframeHeight(window.innerHeight, true);

        // Create a canvas to capture a frame
        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);
        stream.getTracks().forEach(track => track.stop());
        return canvas.toDataURL('image/jpeg', 0.8);
    }
    ''')
    display(js)
    data = eval_js('captureImage()')
    return data

# Function to convert the captured image to an OpenCV format
def data_uri_to_image(data_uri):
    image_data = b64decode(data_uri.split(',')[1])
    np_array = np.frombuffer(image_data, np.uint8)
    image = cv2.imdecode(np_array, cv2.IMREAD_COLOR)
    return image

# Initialize MediaPipe Hand Landmarker
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# Capture an image from the webcam
image_data = capture_image()

# Convert the image to OpenCV format
image = data_uri_to_image(image_data)

# Convert the BGR image to RGB
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Process the image and detect hand landmarks
with mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5) as hands:
    results = hands.process(image_rgb)

    # Draw hand landmarks on the image
    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)
    else:
        print("No hand detected")

# Display the processed image
from google.colab.patches import cv2_imshow
cv2_imshow(image)

import cv2
import numpy as np
import mediapipe as mp
from matplotlib import pyplot as plt
from google.colab.patches import cv2_imshow
from google.colab import files

# Initialize Mediapipe solutions
mp_holistic = mp.solutions.holistic  # Holistic model
mp_drawing = mp.solutions.drawing_utils  # Drawing utilities
mp_face_mesh = mp.solutions.face_mesh  # Face connections

def mediapipe_detection(image, model):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB
    image.flags.writeable = False  # Image is no longer writeable
    results = model.process(image)  # Make prediction
    image.flags.writeable = True  # Image is now writeable
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR
    return image, results

def draw_landmarks(image, results):
    if results.face_landmarks:
        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_face_mesh.FACEMESH_CONTOURS)
    if results.pose_landmarks:
        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
    if results.left_hand_landmarks:
        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
    if results.right_hand_landmarks:
        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

# Upload a video file
uploaded = files.upload()
video_path = next(iter(uploaded))

# Open video
cap = cv2.VideoCapture(video_path)

# Set up Mediapipe model
with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Make detections
        image, results = mediapipe_detection(frame, holistic)

        # Draw landmarks
        draw_landmarks(image, results)

        # Display output
        cv2_imshow(image)

    cap.release()
    cv2.destroyAllWindows()

import cv2
import numpy as np
import mediapipe as mp
from google.colab.patches import cv2_imshow
from google.colab import files

# Initialize Mediapipe FaceMesh
mp_face_mesh = mp.solutions.face_mesh  # Face Mesh model
mp_drawing = mp.solutions.drawing_utils  # Drawing utilities

def mediapipe_detection(image, model):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB
    image.flags.writeable = False  # Improve performance
    results = model.process(image)  # Detect face
    image.flags.writeable = True  # Make image writable again
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR
    return image, results

def draw_face_landmarks(image, results):
    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            mp_drawing.draw_landmarks(
                image, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS)

# Upload a video file
uploaded = files.upload()
video_path = next(iter(uploaded))

# Open video
cap = cv2.VideoCapture(video_path)

# Set up Mediapipe FaceMesh model
with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Make face detections
        image, results = mediapipe_detection(frame, face_mesh)

        # Draw only face landmarks
        draw_face_landmarks(image, results)

        # Display output
        cv2_imshow(image)

    cap.release()
    cv2.destroyAllWindows()



import cv2
import numpy as np
import mediapipe as mp
from google.colab.patches import cv2_imshow
from google.colab import files

# Initialize Mediapipe FaceMesh
mp_face_mesh = mp.solutions.face_mesh
mp_drawing = mp.solutions.drawing_utils

# Upload a video file
uploaded = files.upload()
video_path = next(iter(uploaded))

# Open video
cap = cv2.VideoCapture(video_path)

# Function to get bounding box around the face
def get_face_bounding_box(results, frame_shape):
    h, w, _ = frame_shape
    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            x_min, y_min = w, h
            x_max, y_max = 0, 0

            for landmark in face_landmarks.landmark:
                x, y = int(landmark.x * w), int(landmark.y * h)
                x_min, y_min = min(x_min, x), min(y_min, y)
                x_max, y_max = max(x_max, x), max(y_max, y)

            return max(0, x_min), max(0, y_min), min(w, x_max), min(h, y_max)
    return None

# Set up Mediapipe FaceMesh model
with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Convert BGR to RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb_frame)

        # Get face bounding box
        bbox = get_face_bounding_box(results, frame.shape)

        if bbox:
            x_min, y_min, x_max, y_max = bbox
            cropped_face = frame[y_min:y_max, x_min:x_max]  # Crop the face

            # Show cropped face
            cv2_imshow(cropped_face)

    cap.release()
    cv2.destroyAllWindows()

import cv2
import numpy as np
import mediapipe as mp
from google.colab.patches import cv2_imshow
from google.colab import files

# Initialize Mediapipe FaceMesh
mp_face_mesh = mp.solutions.face_mesh
mp_drawing = mp.solutions.drawing_utils

# Upload a video file
uploaded = files.upload()
video_path = next(iter(uploaded))

# Open video
cap = cv2.VideoCapture(video_path)

# Function to get the outermost face boundary using a convex hull
def get_outermost_face_boundaries(results, frame_shape):
    h, w, _ = frame_shape
    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            points = []

            # Collect all landmark points
            for landmark in face_landmarks.landmark:
                x, y = int(landmark.x * w), int(landmark.y * h)
                points.append((x, y))

            # Get convex hull (outer boundary)
            points = np.array(points)
            hull = cv2.convexHull(points)

            return hull
    return None

# Set up Mediapipe FaceMesh model
with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Convert BGR to RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb_frame)

        # Get face boundary
        hull = get_outermost_face_boundaries(results, frame.shape)

        if hull is not None:
            # Create a black mask
            mask = np.zeros(frame.shape, dtype=np.uint8)

            # Fill the convex hull (face area) with white
            cv2.fillPoly(mask, [hull], (255, 255, 255))

            # Apply mask to keep only the face area
            masked_face = cv2.bitwise_and(frame, mask)

            # Draw the convex hull outline
            cv2.polylines(masked_face, [hull], isClosed=True, color=(0, 255, 0), thickness=2)

            # Draw checkpoints inside the face
            for point in hull:
                x, y = point[0]
                cv2.circle(masked_face, (x, y), 2, (0, 0, 255), -1)  # Red dots

            # Show the masked face
            cv2_imshow(masked_face)

    cap.release()
    cv2.destroyAllWindows()

import cv2
import numpy as np
import mediapipe as mp
from google.colab.patches import cv2_imshow
from google.colab import files

# Initialize Mediapipe FaceMesh
mp_face_mesh = mp.solutions.face_mesh
mp_drawing = mp.solutions.drawing_utils

# Upload a video file
uploaded = files.upload()
video_path = next(iter(uploaded))

# Open video
cap = cv2.VideoCapture(video_path)

# Function to get the outermost face boundary using a convex hull
def get_outermost_face_boundaries(results, frame_shape):
    h, w, _ = frame_shape
    all_landmarks = []  # Store all landmark points
    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            points = []

            # Collect all landmark points
            for landmark in face_landmarks.landmark:
                x, y = int(landmark.x * w), int(landmark.y * h)
                points.append((x, y))
                all_landmarks.append((x, y))  # Save for extraction

            # Get convex hull (outer boundary)
            points = np.array(points)
            hull = cv2.convexHull(points)

            return hull, all_landmarks
    return None, None

# Set up Mediapipe FaceMesh model
with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Convert BGR to RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb_frame)

        # Get face boundary and landmarks
        hull, all_landmarks = get_outermost_face_boundaries(results, frame.shape)

        if hull is not None:
            # Create a black mask
            mask = np.zeros(frame.shape, dtype=np.uint8)

            # Fill the convex hull (face area) with white
            cv2.fillPoly(mask, [hull], (255, 255, 255))

            # Apply mask to keep only the face area
            masked_face = cv2.bitwise_and(frame, mask)

            # Draw the convex hull outline
            cv2.polylines(masked_face, [hull], isClosed=True, color=(0, 255, 0), thickness=2)

            # Draw checkpoints inside the face and extract coordinates
            for x, y in all_landmarks:
                cv2.circle(masked_face, (x, y), 2, (0, 0, 255), -1)  # Red dots
                print(f"Checkpoint: ({x}, {y})")  # Print extracted points

            # Show the masked face
            cv2_imshow(masked_face)

    cap.release()
    cv2.destroyAllWindows()

